{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd45f421",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3400185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9e718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_path=\"c:/Windows/Fonts/malgun.ttf\"\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "matplotlib.rc('font',family=font_name)\n",
    "\n",
    "plt.rcParams['font.size'] = 13  # ê¸°ë³¸ í°íŠ¸ í¬ê¸°\n",
    "plt.rcParams['axes.labelsize'] = 13  # x,yì¶• label í°íŠ¸ í¬ê¸°\n",
    "plt.rcParams['xtick.labelsize'] = 13  # xì¶• ëˆˆê¸ˆ í°íŠ¸ í¬ê¸°\n",
    "plt.rcParams['ytick.labelsize'] = 13  # yì¶• ëˆˆê¸ˆ í°íŠ¸ í¬ê¸°\n",
    "plt.rcParams['legend.fontsize'] = 13  # ë²”ë¡€ í°íŠ¸ í¬ê¸°\n",
    "plt.rcParams['figure.titlesize'] = 15  # figure title í°íŠ¸ í¬ê¸°\n",
    "\n",
    "import colorsys\n",
    "\n",
    "def generate_colors(n):\n",
    "    return [colorsys.hsv_to_rgb(i/n, 0.7, 0.9) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b62ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# GPU ì´ë¦„ ì¶œë ¥\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82f5ae",
   "metadata": {},
   "source": [
    "# constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f3d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKBACK = 28\n",
    "PREDICT = 7\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52ff8c",
   "metadata": {},
   "source": [
    "# def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d32d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ì„±ìˆ˜ê¸° êµ¬ë¶„ ë°ì´í„° ìƒì„± í•¨ìˆ˜\n",
    "def generate_peak_season_data(df):\n",
    "    # ì—…ì¥+ë©”ë‰´ ë‹¨ìœ„ë¡œ ì›”ë³„ í‰ê·  ë§¤ì¶œ ê³„ì‚°\n",
    "    monthly_avg = (\n",
    "        df.groupby([\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\", \"ì›”\"])[\"ë§¤ì¶œìˆ˜ëŸ‰\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # ê° ì¡°í•©ë³„ í‰ê·  ë§¤ì¶œ\n",
    "    base_avg = (\n",
    "        monthly_avg.groupby([\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\"])[\"ë§¤ì¶œìˆ˜ëŸ‰\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"ë§¤ì¶œìˆ˜ëŸ‰\": \"ê¸°ì¤€ë§¤ì¶œ\"})\n",
    "    )\n",
    "\n",
    "    # ë³‘í•© í›„ ì„±ìˆ˜ê¸°/ë¹„ìˆ˜ê¸° êµ¬ë¶„ (ì„ê³„ì¹˜: í‰ê·  ëŒ€ë¹„ 1.3ë°° ì´ìƒ = ì„±ìˆ˜ê¸°, 0.7ë°° ì´í•˜ = ë¹„ìˆ˜ê¸°)\n",
    "    monthly_labeled = pd.merge(monthly_avg, base_avg, on=[\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\"])\n",
    "    monthly_labeled[\"ì„±ìˆ˜ê¸°ì—¬ë¶€\"] = monthly_labeled.apply(\n",
    "        lambda row: \"ì„±ìˆ˜ê¸°\" if row[\"ë§¤ì¶œìˆ˜ëŸ‰\"] >= row[\"ê¸°ì¤€ë§¤ì¶œ\"] * 1.3\n",
    "        else \"ë¹„ìˆ˜ê¸°\" if row[\"ë§¤ì¶œìˆ˜ëŸ‰\"] <= row[\"ê¸°ì¤€ë§¤ì¶œ\"] * 0.7\n",
    "        else \"êµ¬ë¶„ ë¶ˆê°€\",\n",
    "        axis=1\n",
    "    )\n",
    "    # ê²°ê³¼: ì—…ì¥+ë©”ë‰´+ì›”ë³„ ì„±ìˆ˜ê¸° ì •ë³´\n",
    "    return monthly_labeled[[\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\", \"ì›”\", \"ì„±ìˆ˜ê¸°ì—¬ë¶€\"]].copy()\n",
    "\n",
    "\n",
    "def preprocess_sales_data_first(df):\n",
    "    # ë‚ ì§œ ë³€í™˜\n",
    "    df[\"ì˜ì—…ì¼ì\"] = pd.to_datetime(df[\"ì˜ì—…ì¼ì\"])\n",
    "\n",
    "    # ì˜ì—…ì¥ëª…ê³¼ ë©”ë‰´ëª… ë¶„ë¦¬\n",
    "    df[\"ì˜ì—…ì¥ëª…\"] = df[\"ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    df[\"ë©”ë‰´ëª…\"] = df[\"ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    # ìš”ì¼ ë° ì£¼ë§ ì—¬ë¶€\n",
    "    df[\"ì›”\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
    "    df[\"ìš”ì¼\"] = df[\"ì˜ì—…ì¼ì\"].dt.weekday\n",
    "    df[\"ì£¼ë§ì—¬ë¶€\"] = df[\"ìš”ì¼\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_sales_data_second(df, peak_season_map=None):\n",
    "    # ê³µíœ´ì¼ í•˜ë“œì½”ë”© (2023~2025)\n",
    "    holidays_2023_2025 = pd.to_datetime([\n",
    "        # 2023ë…„\n",
    "        \"2023-01-01\", \"2023-01-21\", \"2023-01-22\", \"2023-01-23\", \"2023-03-01\", \"2023-05-05\",\n",
    "        \"2023-05-27\", \"2023-06-06\", \"2023-08-15\", \"2023-09-28\", \"2023-09-29\", \"2023-09-30\",\n",
    "        \"2023-10-03\", \"2023-10-09\", \"2023-12-25\",\n",
    "        # 2024ë…„\n",
    "        \"2024-01-01\", \"2024-02-09\", \"2024-02-10\", \"2024-02-11\", \"2024-03-01\", \"2024-05-05\",\n",
    "        \"2024-05-06\", \"2024-06-06\", \"2024-08-15\", \"2024-09-16\", \"2024-09-17\", \"2024-09-18\",\n",
    "        \"2024-10-03\", \"2024-10-09\", \"2024-12-25\",\n",
    "        # 2025ë…„\n",
    "        \"2025-01-01\", \"2025-01-28\", \"2025-01-29\", \"2025-01-30\", \"2025-03-01\", \"2025-05-05\",\n",
    "        \"2025-06-06\", \"2025-08-15\", \"2025-10-03\", \"2025-10-06\", \"2025-10-07\", \"2025-10-08\",\n",
    "        \"2025-10-09\", \"2025-12-25\"\n",
    "    ])\n",
    "    df[\"ê³µíœ´ì¼ì—¬ë¶€\"] = df[\"ì˜ì—…ì¼ì\"].isin(holidays_2023_2025).astype(int)\n",
    "\n",
    "    # ì„±ìˆ˜ê¸° ì—¬ë¶€ íŒë³„\n",
    "    # ì›” ì»¬ëŸ¼ ìƒì„± í›„ ì„±ìˆ˜ê¸° ë§µê³¼ ë³‘í•©\n",
    "    df[\"ì›”\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
    "    df = df.merge(\n",
    "        peak_season_map,\n",
    "        how=\"left\",\n",
    "        on=[\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\", \"ì›”\"]\n",
    "    )\n",
    "\n",
    "    # ì„±ìˆ˜ê¸°ì—¬ë¶€ ê²°ì¸¡ê°’ì€ \"êµ¬ë¶„ ë¶ˆê°€\" ì²˜ë¦¬\n",
    "    df[\"ì„±ìˆ˜ê¸°ì—¬ë¶€\"] = df[\"ì„±ìˆ˜ê¸°ì—¬ë¶€\"].fillna(\"êµ¬ë¶„ ë¶ˆê°€\")\n",
    "\n",
    "    # time_idx ì¶”ê°€ (ì‹œê³„ì—´ ì¸ë±ìŠ¤)\n",
    "    df = df.sort_values([\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\", \"ì˜ì—…ì¼ì\"])\n",
    "    df[\"time_idx\"] = df.groupby([\"ì˜ì—…ì¥ëª…\", \"ë©”ë‰´ëª…\"]).cumcount()\n",
    "\n",
    "    # group_id ì¶”ê°€\n",
    "    df[\"group_id\"] = df[\"ì˜ì—…ì¥ëª…\"] + \"_\" + df[\"ë©”ë‰´ëª…\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ğŸ”¥ ì´ë™í‰ê·  í”¼ì²˜\n",
    "    for window in [3, 7, 14]:\n",
    "        df[f'ma_{window}'] = df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ë§¤ì¶œìˆ˜ëŸ‰'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "    # ğŸ”¥ lag í”¼ì²˜\n",
    "    for lag in [1, 7, 14]:\n",
    "        df[f'lag_{lag}'] = df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ë§¤ì¶œìˆ˜ëŸ‰'].shift(lag)\n",
    "\n",
    "    # ğŸ”¥ ê³„ì ˆì„± í”¼ì²˜\n",
    "    df['month'] = df['ì˜ì—…ì¼ì'].dt.month\n",
    "    df['day_of_month'] = df['ì˜ì—…ì¼ì'].dt.day\n",
    "    df['quarter'] = df['ì˜ì—…ì¼ì'].dt.quarter\n",
    "\n",
    "    # ğŸ”¥ ìˆœí™˜ì  ì‹œê°„ í”¼ì²˜ (sin/cos ë³€í™˜)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['ìš”ì¼'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['ìš”ì¼'] / 7)\n",
    "\n",
    "    # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de32637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, output_dim=7):\n",
    "        super(MultiOutputLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])  # (B, output_dim)\n",
    "\n",
    "\n",
    "class EnhancedLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=3, output_dim=7, dropout=0.2):\n",
    "        super(EnhancedLSTM, self).__init__()\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : Dropout, Batch Normalization ì¶”ê°€\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : ë‹¤ì¸µ FC ë„¤íŠ¸ì›Œí¬\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # (batch, hidden_dim)\n",
    "\n",
    "        # Batch normalization (2D ì…ë ¥ í•„ìš”)\n",
    "        if last_output.size(0) > 1:  # batch_size > 1ì¼ ë•Œë§Œ\n",
    "            last_output = self.batch_norm(last_output)\n",
    "\n",
    "        x = self.dropout(last_output)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e9c218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(train_df):\n",
    "    trained_models = {}\n",
    "\n",
    "    for store_menu, group in tqdm(train_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']), desc ='Training LSTM'):\n",
    "        store_train = group.sort_values('ì˜ì—…ì¼ì').copy()\n",
    "        if len(store_train) < LOOKBACK + PREDICT:\n",
    "            continue\n",
    "\n",
    "        features = ['ë§¤ì¶œìˆ˜ëŸ‰']\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "        # DataFrameì´ ì•„ë‹Œ numpy ë°°ì—´ë¡œ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬\n",
    "        train_vals = store_train[features].values  # ë¨¼ì € numpyë¡œ ë³€í™˜\n",
    "        train_vals_scaled = scaler.fit_transform(train_vals)  # numpyë¡œ fit\n",
    "        # store_train[features] = scaler.fit_transform(store_train[features])\n",
    "        # train_vals = store_train[features].values  # shape: (N, 1)\n",
    "\n",
    "        # ì‹œí€€ìŠ¤ êµ¬ì„±\n",
    "        X_train, y_train = [], []\n",
    "        for i in range(len(train_vals_scaled) - LOOKBACK - PREDICT + 1):\n",
    "            X_train.append(train_vals_scaled[i:i+LOOKBACK])\n",
    "            y_train.append(train_vals_scaled[i+LOOKBACK:i+LOOKBACK+PREDICT, 0])\n",
    "\n",
    "        # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë¨¼ì € ë³€í™˜\n",
    "        X_train = torch.tensor(np.array(X_train)).float().to(DEVICE)\n",
    "        y_train = torch.tensor(np.array(y_train)).float().to(DEVICE)\n",
    "\n",
    "        model = MultiOutputLSTM(input_dim=1, output_dim=PREDICT).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(EPOCHS):\n",
    "            idx = torch.randperm(len(X_train))\n",
    "            for i in range(0, len(X_train), BATCH_SIZE):\n",
    "                batch_idx = idx[i:i+BATCH_SIZE]\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        trained_models[store_menu] = {\n",
    "            'model': model.eval(),\n",
    "            'scaler': scaler,\n",
    "            'last_sequence': train_vals_scaled[-LOOKBACK:]  # (28, 1)\n",
    "        }\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "def train_lstm_improved(train_df):\n",
    "    trained_models = {}\n",
    "\n",
    "    for store_menu, group in tqdm(train_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']), desc='Training Enhanced LSTM'):\n",
    "        store_train = group.sort_values('ì˜ì—…ì¼ì').copy()\n",
    "        if len(store_train) < LOOKBACK + PREDICT:\n",
    "            continue\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : ë‹¤ì¤‘ í”¼ì²˜ ì‚¬ìš©\n",
    "        features = ['ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€']\n",
    "\n",
    "        # ì„±ìˆ˜ê¸°ì—¬ë¶€ ì›í•« ì¸ì½”ë”©\n",
    "        season_dummies = pd.get_dummies(store_train['ì„±ìˆ˜ê¸°ì—¬ë¶€'], prefix='ì„±ìˆ˜ê¸°')\n",
    "        feature_data = pd.concat([store_train[features], season_dummies], axis=1)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        train_vals_scaled = scaler.fit_transform(feature_data.values)\n",
    "\n",
    "        # ì‹œí€€ìŠ¤ êµ¬ì„±\n",
    "        X_train, y_train = [], []\n",
    "        for i in range(len(train_vals_scaled) - LOOKBACK - PREDICT + 1):\n",
    "            X_train.append(train_vals_scaled[i:i+LOOKBACK])\n",
    "            y_train.append(train_vals_scaled[i+LOOKBACK:i+LOOKBACK+PREDICT, 0])  # ë§¤ì¶œìˆ˜ëŸ‰ë§Œ ì˜ˆì¸¡\n",
    "\n",
    "        X_train = torch.tensor(np.array(X_train)).float().to(DEVICE)\n",
    "        y_train = torch.tensor(np.array(y_train)).float().to(DEVICE)\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : ì…ë ¥ ì°¨ì› ìˆ˜ì •\n",
    "        input_dim = feature_data.shape[1]\n",
    "        model = MultiOutputLSTM(input_dim=input_dim, hidden_dim=128, num_layers=3, output_dim=PREDICT).to(DEVICE)\n",
    "\n",
    "        # âœ… í›ˆë ¨ ì½”ë“œ ì¶”ê°€ (ëˆ„ë½ëœ ë¶€ë¶„)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(EPOCHS):\n",
    "            idx = torch.randperm(len(X_train))\n",
    "            for i in range(0, len(X_train), BATCH_SIZE):\n",
    "                batch_idx = idx[i:i+BATCH_SIZE]\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        trained_models[store_menu] = {\n",
    "            'model': model.eval(),\n",
    "            'scaler': scaler,\n",
    "            'feature_columns': feature_data.columns.tolist(),\n",
    "            'last_sequence': train_vals_scaled[-LOOKBACK:]\n",
    "        }\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "def train_lstm_with_validation(train_df):\n",
    "    trained_models = {}\n",
    "\n",
    "    for store_menu, group in tqdm(train_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']), desc='Training with Validation'):\n",
    "        store_train = group.sort_values('ì˜ì—…ì¼ì').copy()\n",
    "        if len(store_train) < LOOKBACK + PREDICT + 14:  # ê²€ì¦ìš© ë°ì´í„° í™•ë³´\n",
    "            continue\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : Train/Validation ë¶„í• \n",
    "        val_size = 14  # 2ì£¼ì¹˜ ê²€ì¦ ë°ì´í„°\n",
    "        train_data = store_train[:-val_size]\n",
    "        val_data = store_train[-val_size:]\n",
    "\n",
    "        # í”¼ì²˜ ì¤€ë¹„\n",
    "        features = ['ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€']\n",
    "        season_dummies = pd.get_dummies(store_train['ì„±ìˆ˜ê¸°ì—¬ë¶€'], prefix='ì„±ìˆ˜ê¸°')\n",
    "        feature_data = pd.concat([store_train[features], season_dummies], axis=1)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        all_scaled = scaler.fit_transform(feature_data.values)\n",
    "\n",
    "        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "        train_scaled = all_scaled[:-val_size]\n",
    "        val_scaled = all_scaled[-val_size:]\n",
    "\n",
    "        # ì‹œí€€ìŠ¤ ìƒì„±\n",
    "        X_train, y_train = create_sequences(train_scaled, LOOKBACK, PREDICT)\n",
    "        X_val, y_val = create_sequences(val_scaled, LOOKBACK, PREDICT)\n",
    "\n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            continue\n",
    "\n",
    "        # ëª¨ë¸ í›ˆë ¨\n",
    "        model = EnhancedLSTM(input_dim=feature_data.shape[1],\n",
    "                            hidden_dim=128, num_layers=3, output_dim=PREDICT).to(DEVICE)\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§, Early Stopping\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # í›ˆë ¨\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for i in range(0, len(X_train), BATCH_SIZE):\n",
    "                batch_X = X_train[i:i+BATCH_SIZE]\n",
    "                batch_y = y_train[i:i+BATCH_SIZE]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_X)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "\n",
    "                # ğŸ”¥ ê°œì„ : Gradient Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # ê²€ì¦\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(X_val)\n",
    "                val_loss = criterion(val_output, y_val).item()\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Early Stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 20:  # Early stopping\n",
    "                    break\n",
    "\n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³µì›\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "        trained_models[store_menu] = {\n",
    "            'model': model.eval(),\n",
    "            'scaler': scaler,\n",
    "            'feature_columns': feature_data.columns.tolist(),\n",
    "            'last_sequence': all_scaled[-LOOKBACK:]\n",
    "        }\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "def create_sequences(data, lookback, predict):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - predict + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+predict, 0])  # ë§¤ì¶œìˆ˜ëŸ‰ë§Œ ì˜ˆì¸¡\n",
    "\n",
    "    if len(X) > 0:\n",
    "        return torch.tensor(np.array(X)).float().to(DEVICE), torch.tensor(np.array(y)).float().to(DEVICE)\n",
    "    else:\n",
    "        return torch.tensor([]).to(DEVICE), torch.tensor([]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fd49362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(test_df, trained_models, test_prefix: str):\n",
    "    results = []\n",
    "\n",
    "    for store_menu, store_test in test_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']):\n",
    "        key = store_menu\n",
    "        if key not in trained_models:\n",
    "            continue\n",
    "\n",
    "        model = trained_models[key]['model']\n",
    "        scaler = trained_models[key]['scaler']\n",
    "\n",
    "        store_test_sorted = store_test.sort_values('ì˜ì—…ì¼ì')\n",
    "        recent_vals = store_test_sorted['ë§¤ì¶œìˆ˜ëŸ‰'].values[-LOOKBACK:]\n",
    "        if len(recent_vals) < LOOKBACK:\n",
    "            continue\n",
    "\n",
    "        # ì •ê·œí™”\n",
    "        recent_vals = scaler.transform(recent_vals.reshape(-1, 1))\n",
    "        x_input = torch.tensor([recent_vals]).float().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_scaled = model(x_input).squeeze().cpu().numpy()\n",
    "\n",
    "        # ì—­ë³€í™˜\n",
    "        restored = []\n",
    "        for i in range(PREDICT):\n",
    "            dummy = np.zeros((1, 1))\n",
    "            dummy[0, 0] = pred_scaled[i]\n",
    "            restored_val = scaler.inverse_transform(dummy)[0, 0]\n",
    "            restored.append(max(restored_val, 0))\n",
    "\n",
    "        # ì˜ˆì¸¡ì¼ì: TEST_00+1ì¼ ~ TEST_00+7ì¼\n",
    "        pred_dates = [f\"{test_prefix}+{i+1}ì¼\" for i in range(PREDICT)]\n",
    "\n",
    "        for d, val in zip(pred_dates, restored):\n",
    "            results.append({\n",
    "                'ì˜ì—…ì¼ì': d,\n",
    "                'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': store_menu,\n",
    "                'ë§¤ì¶œìˆ˜ëŸ‰': val\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def predict_lstm_improved(test_df, trained_models, test_prefix: str):\n",
    "    results = []\n",
    "\n",
    "    for store_menu, store_test in test_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']):\n",
    "        if store_menu not in trained_models:\n",
    "            continue\n",
    "\n",
    "        model_info = trained_models[store_menu]\n",
    "        model = model_info['model']\n",
    "        scaler = model_info['scaler']\n",
    "        feature_columns = model_info['feature_columns']\n",
    "\n",
    "        store_test_sorted = store_test.sort_values('ì˜ì—…ì¼ì')\n",
    "\n",
    "        # ğŸ”¥ ê°œì„ : ëª¨ë“  í”¼ì²˜ ì‚¬ìš©\n",
    "        # í”¼ì²˜ ì¬êµ¬ì„± (í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ìˆœì„œ)\n",
    "        features = ['ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€']\n",
    "        season_dummies = pd.get_dummies(store_test_sorted['ì„±ìˆ˜ê¸°ì—¬ë¶€'], prefix='ì„±ìˆ˜ê¸°')\n",
    "\n",
    "        # í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
    "        for col in feature_columns:\n",
    "            if col not in season_dummies.columns and col not in features:\n",
    "                season_dummies[col] = 0\n",
    "\n",
    "        feature_data = pd.concat([store_test_sorted[features], season_dummies[feature_columns[len(features):]]], axis=1)\n",
    "        feature_data = feature_data[feature_columns]  # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
    "\n",
    "        if len(feature_data) < LOOKBACK:\n",
    "            continue\n",
    "\n",
    "        # ì •ê·œí™”\n",
    "        recent_vals = scaler.transform(feature_data.values[-LOOKBACK:])\n",
    "        x_input = torch.tensor([recent_vals]).float().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_scaled = model(x_input).squeeze().cpu().numpy()\n",
    "\n",
    "        # ì—­ë³€í™˜ (ë§¤ì¶œìˆ˜ëŸ‰ë§Œ)\n",
    "        restored = []\n",
    "        for i in range(PREDICT):\n",
    "            dummy = np.zeros((1, len(feature_columns)))\n",
    "            dummy[0, 0] = pred_scaled[i]  # ì²« ë²ˆì§¸ê°€ ë§¤ì¶œìˆ˜ëŸ‰\n",
    "            restored_val = scaler.inverse_transform(dummy)[0, 0]\n",
    "            restored.append(max(restored_val, 0))\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        pred_dates = [f\"{test_prefix}+{i+1}ì¼\" for i in range(PREDICT)]\n",
    "        for d, val in zip(pred_dates, restored):\n",
    "            results.append({\n",
    "                'ì˜ì—…ì¼ì': d,\n",
    "                'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': store_menu,\n",
    "                'ë§¤ì¶œìˆ˜ëŸ‰': val\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad5b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_submission_format(pred_df: pd.DataFrame, sample_submission: pd.DataFrame):\n",
    "    # (ì˜ì—…ì¼ì, ë©”ë‰´) â†’ ë§¤ì¶œìˆ˜ëŸ‰ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "    pred_dict = dict(zip(\n",
    "        zip(pred_df['ì˜ì—…ì¼ì'], pred_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']),\n",
    "        pred_df['ë§¤ì¶œìˆ˜ëŸ‰']\n",
    "    ))\n",
    "\n",
    "    final_df = sample_submission.copy()\n",
    "\n",
    "    for row_idx in final_df.index:\n",
    "        date = final_df.loc[row_idx, 'ì˜ì—…ì¼ì']\n",
    "        for col in final_df.columns[1:]:  # ë©”ë‰´ëª…ë“¤\n",
    "            final_df.loc[row_idx, col] = pred_dict.get((date, col), 0)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92bb0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smape(actual, predicted):\n",
    "    \"\"\"SMAPE ê³„ì‚° í•¨ìˆ˜\"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "\n",
    "    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    denominator = (np.abs(actual) + np.abs(predicted)) / 2\n",
    "    denominator = np.where(denominator == 0, 1e-8, denominator)\n",
    "\n",
    "    smape = np.abs(actual - predicted) / denominator\n",
    "    return np.mean(smape) * 100\n",
    "\n",
    "def evaluate_model_performance(train_df, trained_models, validation_days=7):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
    "    Args:\n",
    "        train_df: ì „ì²´ í›ˆë ¨ ë°ì´í„°\n",
    "        trained_models: í›ˆë ¨ëœ ëª¨ë¸ë“¤\n",
    "        validation_days: ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ë§ˆì§€ë§‰ ë©°ì¹ ì˜ ë°ì´í„°\n",
    "    \"\"\"\n",
    "    store_performances = []\n",
    "    store_weights = {}\n",
    "\n",
    "    print(\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
    "\n",
    "    for store_menu, group in tqdm(train_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']), desc='í‰ê°€ ì§„í–‰'):\n",
    "        if store_menu not in trained_models:\n",
    "            continue\n",
    "\n",
    "        store_data = group.sort_values('ì˜ì—…ì¼ì').copy()\n",
    "\n",
    "        # ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° ìŠ¤í‚µ\n",
    "        if len(store_data) < LOOKBACK + validation_days:\n",
    "            continue\n",
    "\n",
    "        # í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "        train_split = store_data[:-validation_days]\n",
    "        validation_split = store_data[-validation_days:]\n",
    "\n",
    "        # ì‹¤ì œ ê²€ì¦ ë°ì´í„° ê°’\n",
    "        actual_values = validation_split['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
    "\n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        model = trained_models[store_menu]['model']\n",
    "        scaler = trained_models[store_menu]['scaler']\n",
    "\n",
    "        # ê²€ì¦ ì‹œì‘ì ì˜ ì´ì „ LOOKBACK ì¼ìˆ˜ ë°ì´í„° ì‚¬ìš©\n",
    "        recent_vals = train_split['ë§¤ì¶œìˆ˜ëŸ‰'].values[-LOOKBACK:]\n",
    "\n",
    "        if len(recent_vals) < LOOKBACK:\n",
    "            continue\n",
    "\n",
    "        # ì •ê·œí™”\n",
    "        recent_vals_scaled = scaler.transform(recent_vals.reshape(-1, 1))\n",
    "        x_input = torch.tensor([recent_vals_scaled]).float().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_scaled = model(x_input).squeeze().cpu().numpy()\n",
    "\n",
    "        # ì—­ë³€í™˜\n",
    "        predicted_values = []\n",
    "        for i in range(min(PREDICT, validation_days)):\n",
    "            dummy = np.zeros((1, 1))\n",
    "            dummy[0, 0] = pred_scaled[i]\n",
    "            restored_val = scaler.inverse_transform(dummy)[0, 0]\n",
    "            predicted_values.append(max(restored_val, 0))\n",
    "\n",
    "        # validation_daysì™€ PREDICT ì¤‘ ì‘ì€ ê°’ë§Œí¼ë§Œ í‰ê°€\n",
    "        eval_days = min(validation_days, PREDICT, len(actual_values))\n",
    "        actual_eval = actual_values[:eval_days]\n",
    "        predicted_eval = predicted_values[:eval_days]\n",
    "\n",
    "        # SMAPE ê³„ì‚°\n",
    "        smape = calculate_smape(actual_eval, predicted_eval)\n",
    "\n",
    "        # ë°ì´í„° ì–‘ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë°ì´í„°ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)\n",
    "        zero_count = (store_data['ë§¤ì¶œìˆ˜ëŸ‰'] == 0).sum()\n",
    "        total_count = len(store_data)\n",
    "        zero_ratio = zero_count / total_count\n",
    "\n",
    "        # 0 ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ë†’ì•„ì§€ë„ë¡ ì„¤ì •\n",
    "        # ìµœì†Œ ê°€ì¤‘ì¹˜ë¥¼ 1ë¡œ, ìµœëŒ€ ê°€ì¤‘ì¹˜ë¥¼ 10ìœ¼ë¡œ ì œí•œ\n",
    "        weight = 1 / (1 - zero_ratio + 0.1)  # 0.1ì„ ë”í•´ì„œ ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€\n",
    "        weight = min(weight, 10)  # ìµœëŒ€ ê°€ì¤‘ì¹˜ ì œí•œ\n",
    "\n",
    "        store_performances.append({\n",
    "            'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': store_menu,\n",
    "            'SMAPE': smape,\n",
    "            'Weight': weight,\n",
    "            'Data_Count': total_count,\n",
    "            'Zero_Count': zero_count,\n",
    "            'Zero_Ratio': zero_ratio,\n",
    "            'Weighted_SMAPE': smape * weight\n",
    "        })\n",
    "\n",
    "        store_weights[store_menu] = weight\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    performance_df = pd.DataFrame(store_performances)\n",
    "\n",
    "    if len(performance_df) == 0:\n",
    "        print(\"í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "\n",
    "    # ì „ì²´ ê°€ì¤‘ í‰ê·  SMAPE ê³„ì‚°\n",
    "    total_weighted_smape = performance_df['Weighted_SMAPE'].sum() / performance_df['Weight'].sum()\n",
    "\n",
    "    # í†µê³„ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"\\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ===\")\n",
    "    print(f\"í‰ê°€ ëŒ€ìƒ ì—…ì¥ ìˆ˜: {len(performance_df)}\")\n",
    "    print(f\"í‰ê·  SMAPE: {performance_df['SMAPE'].mean():.4f}\")\n",
    "    print(f\"ê°€ì¤‘ í‰ê·  SMAPE: {total_weighted_smape:.4f}\")\n",
    "    print(f\"SMAPE í‘œì¤€í¸ì°¨: {performance_df['SMAPE'].std():.4f}\")\n",
    "    print(f\"ìµœê³  ì„±ëŠ¥ (lowest SMAPE): {performance_df['SMAPE'].min():.4f}\")\n",
    "    print(f\"ìµœì•… ì„±ëŠ¥ (highest SMAPE): {performance_df['SMAPE'].max():.4f}\")\n",
    "    print(f\"í‰ê·  0 ë¹„ìœ¨: {performance_df['Zero_Ratio'].mean():.4f}\")\n",
    "    print(f\"í‰ê·  ê°€ì¤‘ì¹˜: {performance_df['Weight'].mean():.4f}\")\n",
    "\n",
    "    # ìƒìœ„/í•˜ìœ„ ì„±ëŠ¥ ì—…ì¥ ì¶œë ¥\n",
    "    print(f\"\\n=== ìƒìœ„ 10ê°œ ì—…ì¥ (ë‚®ì€ SMAPE) ===\")\n",
    "    top_performers = performance_df.nsmallest(10, 'SMAPE')\n",
    "    for _, row in top_performers.iterrows():\n",
    "        print(f\"{row['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']}: SMAPE={row['SMAPE']:.4f}, 0 ë¹„ìœ¨={row['Zero_Ratio']:.4f}\")\n",
    "\n",
    "    print(f\"\\n=== í•˜ìœ„ 10ê°œ ì—…ì¥ (ë†’ì€ SMAPE) ===\")\n",
    "    worst_performers = performance_df.nlargest(10, 'SMAPE')\n",
    "    for _, row in worst_performers.iterrows():\n",
    "        print(f\"{row['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']}: SMAPE={row['SMAPE']:.4f}, 0 ë¹„ìœ¨={row['Zero_Ratio']:.4f}\")\n",
    "\n",
    "    return performance_df, total_weighted_smape\n",
    "\n",
    "def evaluate_model_performance_improved(train_df, trained_models, validation_days=7):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜ (ë‹¤ì¤‘ í”¼ì²˜ ì§€ì›)\n",
    "    \"\"\"\n",
    "    store_performances = []\n",
    "\n",
    "    print(\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
    "\n",
    "    for store_menu, group in tqdm(train_df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']), desc='í‰ê°€ ì§„í–‰'):\n",
    "        if store_menu not in trained_models:\n",
    "            continue\n",
    "\n",
    "        store_data = group.sort_values('ì˜ì—…ì¼ì').copy()\n",
    "\n",
    "        # ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° ìŠ¤í‚µ\n",
    "        if len(store_data) < LOOKBACK + validation_days:\n",
    "            continue\n",
    "\n",
    "        # í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "        train_split = store_data[:-validation_days]\n",
    "        validation_split = store_data[-validation_days:]\n",
    "\n",
    "        # ì‹¤ì œ ê²€ì¦ ë°ì´í„° ê°’\n",
    "        actual_values = validation_split['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
    "\n",
    "        # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        model_info = trained_models[store_menu]\n",
    "        model = model_info['model']\n",
    "        scaler = model_info['scaler']\n",
    "        feature_columns = model_info['feature_columns']\n",
    "\n",
    "        # âœ… ë‹¤ì¤‘ í”¼ì²˜ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        # í”¼ì²˜ ì¬êµ¬ì„±\n",
    "        features = ['ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€']\n",
    "        season_dummies = pd.get_dummies(train_split['ì„±ìˆ˜ê¸°ì—¬ë¶€'], prefix='ì„±ìˆ˜ê¸°')\n",
    "\n",
    "        # í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
    "        for col in feature_columns:\n",
    "            if col not in season_dummies.columns and col not in features:\n",
    "                season_dummies[col] = 0\n",
    "\n",
    "        feature_data = pd.concat([train_split[features], season_dummies[feature_columns[len(features):]]], axis=1)\n",
    "        feature_data = feature_data[feature_columns]  # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
    "\n",
    "        if len(feature_data) < LOOKBACK:\n",
    "            continue\n",
    "\n",
    "        # ì •ê·œí™”\n",
    "        recent_vals = scaler.transform(feature_data.values[-LOOKBACK:])\n",
    "        x_input = torch.tensor([recent_vals]).float().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_scaled = model(x_input).squeeze().cpu().numpy()\n",
    "\n",
    "        # ì—­ë³€í™˜\n",
    "        predicted_values = []\n",
    "        for i in range(min(PREDICT, validation_days)):\n",
    "            dummy = np.zeros((1, len(feature_columns)))\n",
    "            dummy[0, 0] = pred_scaled[i]  # ì²« ë²ˆì§¸ê°€ ë§¤ì¶œìˆ˜ëŸ‰\n",
    "            restored_val = scaler.inverse_transform(dummy)[0, 0]\n",
    "            predicted_values.append(max(restored_val, 0))\n",
    "\n",
    "        # validation_daysì™€ PREDICT ì¤‘ ì‘ì€ ê°’ë§Œí¼ë§Œ í‰ê°€\n",
    "        eval_days = min(validation_days, PREDICT, len(actual_values))\n",
    "        actual_eval = actual_values[:eval_days]\n",
    "        predicted_eval = predicted_values[:eval_days]\n",
    "\n",
    "        # SMAPE ê³„ì‚°\n",
    "        smape = calculate_smape(actual_eval, predicted_eval)\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "        zero_count = (store_data['ë§¤ì¶œìˆ˜ëŸ‰'] == 0).sum()\n",
    "        total_count = len(store_data)\n",
    "        zero_ratio = zero_count / total_count\n",
    "        weight = 1 / (1 - zero_ratio + 0.1)\n",
    "        weight = min(weight, 10)\n",
    "\n",
    "        store_performances.append({\n",
    "            'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': store_menu,\n",
    "            'SMAPE': smape,\n",
    "            'Weight': weight,\n",
    "            'Data_Count': total_count,\n",
    "            'Zero_Count': zero_count,\n",
    "            'Zero_Ratio': zero_ratio,\n",
    "            'Weighted_SMAPE': smape * weight\n",
    "        })\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    performance_df = pd.DataFrame(store_performances)\n",
    "\n",
    "    if len(performance_df) == 0:\n",
    "        print(\"í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "\n",
    "    # ì „ì²´ ê°€ì¤‘ í‰ê·  SMAPE ê³„ì‚°\n",
    "    total_weighted_smape = performance_df['Weighted_SMAPE'].sum() / performance_df['Weight'].sum()\n",
    "\n",
    "    # í†µê³„ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"\\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ===\")\n",
    "    print(f\"í‰ê°€ ëŒ€ìƒ ì—…ì¥ ìˆ˜: {len(performance_df)}\")\n",
    "    print(f\"í‰ê·  SMAPE: {performance_df['SMAPE'].mean():.4f}\")\n",
    "    print(f\"ê°€ì¤‘ í‰ê·  SMAPE: {total_weighted_smape:.4f}\")\n",
    "    print(f\"SMAPE í‘œì¤€í¸ì°¨: {performance_df['SMAPE'].std():.4f}\")\n",
    "    print(f\"ìµœê³  ì„±ëŠ¥ (lowest SMAPE): {performance_df['SMAPE'].min():.4f}\")\n",
    "    print(f\"ìµœì•… ì„±ëŠ¥ (highest SMAPE): {performance_df['SMAPE'].max():.4f}\")\n",
    "    print(f\"í‰ê·  0 ë¹„ìœ¨: {performance_df['Zero_Ratio'].mean():.4f}\")\n",
    "    print(f\"í‰ê·  ê°€ì¤‘ì¹˜: {performance_df['Weight'].mean():.4f}\")\n",
    "\n",
    "    return performance_df, total_weighted_smape\n",
    "\n",
    "def plot_performance_distribution(performance_df):\n",
    "    \"\"\"ì„±ëŠ¥ ë¶„í¬ ì‹œê°í™” (ì„ íƒì‚¬í•­)\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "        # SMAPE ë¶„í¬\n",
    "        axes[0, 0].hist(performance_df['SMAPE'], bins=30, alpha=0.7)\n",
    "        axes[0, 0].set_title('SMAPE ë¶„í¬')\n",
    "        axes[0, 0].set_xlabel('SMAPE')\n",
    "        axes[0, 0].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "        # 0 ë¹„ìœ¨ vs SMAPE\n",
    "        axes[0, 1].scatter(performance_df['Zero_Ratio'], performance_df['SMAPE'], alpha=0.6)\n",
    "        axes[0, 1].set_title('0 ë¹„ìœ¨ vs SMAPE')\n",
    "        axes[0, 1].set_xlabel('0 ë¹„ìœ¨')\n",
    "        axes[0, 1].set_ylabel('SMAPE')\n",
    "\n",
    "        # 0 ë¹„ìœ¨ ë¶„í¬\n",
    "        axes[0, 2].hist(performance_df['Zero_Ratio'], bins=30, alpha=0.7)\n",
    "        axes[0, 2].set_title('0 ë¹„ìœ¨ ë¶„í¬')\n",
    "        axes[0, 2].set_xlabel('0 ë¹„ìœ¨')\n",
    "        axes[0, 2].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ ë¶„í¬\n",
    "        axes[1, 0].hist(performance_df['Weight'], bins=30, alpha=0.7)\n",
    "        axes[1, 0].set_title('ê°€ì¤‘ì¹˜ ë¶„í¬')\n",
    "        axes[1, 0].set_xlabel('ê°€ì¤‘ì¹˜')\n",
    "        axes[1, 0].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "        # 0 ë¹„ìœ¨ vs ê°€ì¤‘ì¹˜\n",
    "        axes[1, 1].scatter(performance_df['Zero_Ratio'], performance_df['Weight'], alpha=0.6)\n",
    "        axes[1, 1].set_title('0 ë¹„ìœ¨ vs ê°€ì¤‘ì¹˜')\n",
    "        axes[1, 1].set_xlabel('0 ë¹„ìœ¨')\n",
    "        axes[1, 1].set_ylabel('ê°€ì¤‘ì¹˜')\n",
    "\n",
    "        # ê°€ì¤‘ SMAPE ë¶„í¬\n",
    "        axes[1, 1].hist(performance_df['Weighted_SMAPE'], bins=30, alpha=0.7)\n",
    "        axes[1, 1].set_title('ê°€ì¤‘ SMAPE ë¶„í¬')\n",
    "        axes[1, 1].set_xlabel('ê°€ì¤‘ SMAPE')\n",
    "        axes[1, 1].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f104c94",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e273c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102676 entries, 0 to 102675\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count   Dtype         \n",
      "---  ------    --------------   -----         \n",
      " 0   ì˜ì—…ì¼ì      102676 non-null  datetime64[ns]\n",
      " 1   ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…  102676 non-null  object        \n",
      " 2   ë§¤ì¶œìˆ˜ëŸ‰      102676 non-null  int64         \n",
      " 3   ì˜ì—…ì¥ëª…      102676 non-null  object        \n",
      " 4   ë©”ë‰´ëª…       102676 non-null  object        \n",
      " 5   ì›”         102676 non-null  int32         \n",
      " 6   ìš”ì¼        102676 non-null  int32         \n",
      " 7   ì£¼ë§ì—¬ë¶€      102676 non-null  int64         \n",
      " 8   ê³µíœ´ì¼ì—¬ë¶€     102676 non-null  int64         \n",
      " 9   ì„±ìˆ˜ê¸°ì—¬ë¶€     102676 non-null  object        \n",
      " 10  time_idx  102676 non-null  int64         \n",
      " 11  group_id  102676 non-null  object        \n",
      "dtypes: datetime64[ns](1), int32(2), int64(4), object(5)\n",
      "memory usage: 8.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train = preprocess_sales_data_first(train)\n",
    "# ì„±ìˆ˜ê¸° êµ¬ë¶„ ë°ì´í„° ìƒì„±\n",
    "peak_season_map = generate_peak_season_data(train)\n",
    "train = preprocess_sales_data_second(train, peak_season_map)\n",
    "\n",
    "list_name = train[\"ì˜ì—…ì¥ëª…\"].explode().unique()\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90f07fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…</th>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì˜ì—…ì¼ì</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2023-01-02 00:00:00</td>\n",
       "      <td>2023-01-03 00:00:00</td>\n",
       "      <td>2023-01-04 00:00:00</td>\n",
       "      <td>2023-01-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì˜ì—…ì¥ëª…</th>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ</td>\n",
       "      <td>ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë©”ë‰´ëª…</th>\n",
       "      <td>1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "      <td>1ì¸ ìˆ˜ì €ì„¸íŠ¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë§¤ì¶œìˆ˜ëŸ‰</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ìš”ì¼</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì£¼ë§ì—¬ë¶€</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ê³µíœ´ì¼ì—¬ë¶€</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì„±ìˆ˜ê¸°ì—¬ë¶€</th>\n",
       "      <td>ë¹„ìˆ˜ê¸°</td>\n",
       "      <td>ë¹„ìˆ˜ê¸°</td>\n",
       "      <td>ë¹„ìˆ˜ê¸°</td>\n",
       "      <td>ë¹„ìˆ˜ê¸°</td>\n",
       "      <td>ë¹„ìˆ˜ê¸°</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                    1                    2  \\\n",
       "ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…   ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸   ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸   ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸   \n",
       "ì˜ì—…ì¼ì      2023-01-01 00:00:00  2023-01-02 00:00:00  2023-01-03 00:00:00   \n",
       "ì˜ì—…ì¥ëª…               ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ           ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ           ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ   \n",
       "ë©”ë‰´ëª…                   1ì¸ ìˆ˜ì €ì„¸íŠ¸              1ì¸ ìˆ˜ì €ì„¸íŠ¸              1ì¸ ìˆ˜ì €ì„¸íŠ¸   \n",
       "ë§¤ì¶œìˆ˜ëŸ‰                        0                    0                    0   \n",
       "ìš”ì¼                          6                    0                    1   \n",
       "ì£¼ë§ì—¬ë¶€                        1                    0                    0   \n",
       "ê³µíœ´ì¼ì—¬ë¶€                       1                    0                    0   \n",
       "ì„±ìˆ˜ê¸°ì—¬ë¶€                     ë¹„ìˆ˜ê¸°                  ë¹„ìˆ˜ê¸°                  ë¹„ìˆ˜ê¸°   \n",
       "\n",
       "                            3                    4  \n",
       "ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…   ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸   ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_1ì¸ ìˆ˜ì €ì„¸íŠ¸  \n",
       "ì˜ì—…ì¼ì      2023-01-04 00:00:00  2023-01-05 00:00:00  \n",
       "ì˜ì—…ì¥ëª…               ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ           ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ  \n",
       "ë©”ë‰´ëª…                   1ì¸ ìˆ˜ì €ì„¸íŠ¸              1ì¸ ìˆ˜ì €ì„¸íŠ¸  \n",
       "ë§¤ì¶œìˆ˜ëŸ‰                        0                    0  \n",
       "ìš”ì¼                          2                    3  \n",
       "ì£¼ë§ì—¬ë¶€                        0                    0  \n",
       "ê³µíœ´ì¼ì—¬ë¶€                       0                    0  \n",
       "ì„±ìˆ˜ê¸°ì—¬ë¶€                     ë¹„ìˆ˜ê¸°                  ë¹„ìˆ˜ê¸°  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'ì˜ì—…ì¼ì', 'ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€', 'ì„±ìˆ˜ê¸°ì—¬ë¶€']].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "746cc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"data/train_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4ac49",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2f6e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with Validation:   0%|          | 0/193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 193/193 [00:00<00:00, 243.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ\n",
    "# trained_models = train_lstm_improved(train[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'ì˜ì—…ì¼ì', 'ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€', 'ì„±ìˆ˜ê¸°ì—¬ë¶€']])\n",
    "trained_models = train_lstm_with_validation(train[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'ì˜ì—…ì¼ì', 'ì˜ì—…ì¥ëª…', 'ë©”ë‰´ëª…', 'ë§¤ì¶œìˆ˜ëŸ‰', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€', 'ê³µíœ´ì¼ì—¬ë¶€', 'ì„±ìˆ˜ê¸°ì—¬ë¶€']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4979d",
   "metadata": {},
   "source": [
    "# performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1c37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‰ê°€ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 193/193 [00:00<00:00, 22281.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‰ê°€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ í›„\n",
    "performance_df, weighted_smape = evaluate_model_performance_improved(train, trained_models)\n",
    "\n",
    "# ì‹œê°í™” (ì„ íƒì‚¬í•­)\n",
    "if performance_df is not None:\n",
    "    plot_performance_distribution(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e1b2b",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf41875",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "# all_preds = []\n",
    "\n",
    "# # ëª¨ë“  test_*.csv ìˆœíšŒ\n",
    "# test_files = sorted(glob.glob('data/TEST_*.csv'))\n",
    "\n",
    "# for path in test_files:\n",
    "#     test_df = pd.read_csv(path)\n",
    "#     test_df = preprocess_sales_data(test_df)\n",
    "\n",
    "#     # íŒŒì¼ëª…ì—ì„œ ì ‘ë‘ì–´ ì¶”ì¶œ (ì˜ˆ: TEST_00)\n",
    "#     filename = os.path.basename(path)\n",
    "#     test_prefix = re.search(r'(TEST_\\d+)', filename).group(1)\n",
    "\n",
    "#     pred_df = predict_lstm(test_df, trained_models, test_prefix)\n",
    "#     all_preds.append(pred_df)\n",
    "\n",
    "# full_pred_df = pd.concat(all_preds, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
